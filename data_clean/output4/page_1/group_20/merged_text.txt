We first evaluate the effectiveness of
PREDATOR in reproducing known vulnerabilities and com-
pare it with other fuzzers. For each application, we search and
select the reported code locations of all known vulnerabilities
as targets for PREDATOR. We run the crawler of Witcher
to collect URLs of the target vulnerabilities for Witcher
and Witcher+. We adopt the settings recommended in the
paper [16], providing the crawler with valid credentials and
run it for four hours. For WebFuzz, we manually browse the
applications to collect URLs and ensure it can successfully
access all pages and forms related to the target vulnerabilities.
We then exclude irrelevant URLs that do not lead to the
target vulnerable locations and assign an equal time budget
for each of the remaining URLs. This measure allows all
tools to focus on testing target vulnerabilities instead of
exploring unrelated execution paths, thereby ensuring a fair
comparison. If we consider a scenario where no irrelevant
URLs are excluded and an equal time budget is allocated
for each URL, several challenges would arise. A prolonged
budget may result in excessive time spent on irrelevant URLs,
while a budget that is too short may prevent thorough testing
of vulnerability-related URLs. If we set the same total time
budget for each tool for analyzing one application, the actual
testing duration on vulnerability-related URLs may vary
significantly across different tools. In both scenarios, these
discrepancies would negatively impact the fairness of the
comparison.
Results. Table 2 summarizes the results on synthetic
test suites and Table 3 presents the results on real-world
applications. Combining two datasets, PREDATOR effectively
identified 32 SQL injection vulnerabilities, 5 command
injection vulnerabilities, and 12 XSS vulnerabilities across
all 17 applications. Witcher successfully triggered 16 SQLi
vulnerabilities and 1 CMDi vulnerability, respectively. Ac-
cording to the original papers, Atropos successfully triggered
21 SQLi vulnerabilities and 6 CMDi vulnerabilities in the
synthetic test suites [18]; Cefuzz triggered all 5 CMDi
vulnerabilities in the synthetic test suites [30]. WebFuzz suc-
cessfully identified 9 XSS vulnerabilities, whereas Witcher+
only detected 2. On real-world applications, PREDATOR
detected all vulnerabilities that other tools could detect, and
additionally discovered 9 vulnerabilities that other tools failed
to detect.
COMparison With vylttner, he perrormance improvements
of PREDATOR primarily result from the entry URL identi-
fication and targeted input corpus construction techniques.
For instance, we observed that Witcher struggled to collect
effective entry URLs for some applications, notably bWAPP,
OpenEMR, and rConfig. We conducted additional crawling
for several applications using Witcher’s crawler. However,
after the crawler stopped or reached the time budget, we
observed that the results never included an effective entry
URL for certain vulnerabilities. This is consistent with the
findings in the paper [16].
Comparison with Atropos. The performances of PREDATOR
and Atropos regarding bWAPP and XVWA show marginal
differences. Discrepancy is primarily observed when eval-
uating DVWA. This is due to the inherent limitation of
Witcher on which PREDATOR depends—it cannot carry hid-
den random CSRF tokens, rendering PREDATOR ineffective
in testing DVWA. In contrast, Atropos operates on a snapshot-
TABLE 2: The evaluation results on synthetic test suites. We list the total number of vulnerabilities by type, in the format of
SQLi + CMDi + XSS. As the source code is not available at the time of writing, we borrow the detection results from the papers of
Atropos and Cefuzz. "-" denotes that no detection results are available.
TABLE 3: The evaluation results on real-world applications. We list the total number of vulnerabilities by type, in the format of
SQLi + CMDi + XSS. +We remove the hidden tokens in WeBid for a comparison between PREDATOR and Witcher.
based approach, making the random tokens deterministic
during fuzzing [18]. We cite Atropos’ detection results in the
best-case scenario as the ground truths used in the evaluation
of Atropos and PREDATOR are not identical. When evaluating
PREDATOR on reproducing known vulnerabilities, we did
not count any true-positive vulnerabilities outside the ground
truth, whereas Atropos did. The current testing results for
Atropos are limited to three synthetic test suites, which
hinders our ability to perform a comprehensive comparison
on real-world applications.
Comparison with YWebdruzzZ ine performance of
PREDATOR is slightly better than that of WebFuzz. We
noticed that WebFuzz occasionally got stuck on certain
pages, preventing further progress. For instance, when testing
Hospital Management System, despite providing valid login
credentials, WebFuzz consistently attempted to visit inacces-
sible pages. This likely represents a design or implementation
flaw, leading it to waste considerable time exploring irrelevant
paths. When testing other applications, its performance
was satisfactory yet remained marginally inferior to that
of PREDATOR.

False Negatives. We analyze the reasons behind the
false negatives. In bWAPP, 6 SQLi vulnerabilities were not
detected, 3 of which were due to the lack of oracle for sqlite.
This could be easily addressed by detecting the correspond-
ing error feedback messages. However, we refrained from
improving it to maintain fairness in comparison with Witcher.
This is one of the reasons why PREDATOR underperformed
Atropos in detecting SQLi vulnerabilities in bWAPP. The
remaining 3 vulnerabilities could not be triggered by our
current prototype due to the presence of CAPTCHAs and the
requirement of specific User Agent (UA) header values. For
a fair comparison with Atropos, we did not remove hidden
CSRF tokens in DVWA. As Witcher and PREDATOR cannot
handle hidden tokens, they failed to trigger any vulnerabilities
in DVWA. The vulnerabilities undetected in OpenEMR could
be identified by concurrently fuzzing with multiple cores or
increasing the time budget. Moreover, we discovered that
PREDATOR was unable to detect any vulnerabilities within
WordPress, Piwigo and Joomla due to the failure of entry
URL identification. For example, they dynamically includes
server-side files based on the values of query strings, which
are impractical to identify using static analysis methods.
During the XSS vulnerability detection, we noted that the
absence of injected payloads in the source code of some
pages prevented the matching of the attack string. As a
result, PREDATOR failed to detect the vulnerabilities. This
is an implementation limitation of the matching-based XSS
detection method.
False Positives. All 6 false positives were reported during
the XSS vulnerability detection. This is possibly due to the
high false-positive rate of the string matching-based XSS
detection method. This risk of false positives arises whenever
an application outputs user input on a page. Addressing this
issue could involve utilizing a JavaScript engine to render
the responses.
Uta. EUUCICMCYy, Yye 1UruIer COMIPare FREVDALON WIT
Witcher on the efficiency. The comparison was exclusively
made with Witcher because PREDATOR is implemented atop
Witcher, and it significantly differs from the other tools.
Additionally, Cefuzz neither has available source code nor an
evaluation of time to exposure in its paper, thus we are unable
to compare PREDATOR with it. While we acknowledge that
comparing PREDATOR, a directed fuzzer, to Witcher may
seem unfair, it is important to note that our intention is not
to criticize Witcher. Rather, our goal is to emphasize the
advantages of directed fuzzing for web applications. We
select vulnerabilities in real-world applications that could be
successfully triggered and allocate a 24-hour time budget
for each one and run the experiment ten times. Moreover,
to enhance fairness as much as possible, we provide both
tools with the same input corpus and entry URLs, and set
the initial seeds to be empty.
Results. The average time to exposure (u7TTE) is shown
in Table 4. We used the Mann-Whitney U test to assess
the statistical significance of our experimental results. In
22 out of 25 cases, the results are statistically significant,
with p-values less than 0.05. In the other 3 cases, both tools
either failed to detect the vulnerability, resulting in a 24-
hour TTE, or they triggered the vulnerabilities within similar
timeframes in multiple trials, yielding higher p-values.
PREDATOR outperformed Witcher in 21 out of 25 cases
(highlighted in bold in the table), with p-values less than 0.05.
PREDATOR demonstrated shorter wTTE and successfully
triggered more vulnerabilities in ten attempts. Specifically,
both PREDATOR and Witcher triggered the vulnerability
CVE-2020-29283 in all ten attempts, with PREDATOR
achieving a ITE that was 43.8 times shorter. We observed
that Witcher’s 7TE is marginally lower when detecting
CVE-2020-15713. This can be attributed to the simplicity of
the target vulnerability. Both tools triggered it in seconds, but
the throughput of PREDATOR was relatively lower. Note that
in this experiment we provided Witcher with the entry URLs
and input corpus obtained by PREDATOR. Consequently,
Witcher triggered some vulnerabilities that could not be
identified when using its crawler in Table 3. However, the
input corpus, derived from analyzing the target script, yielded
a greater number of potential parameters than those obtained
using a crawler, thereby expanding the search space. Witcher
was unable to efficiently identify which parameters could
be used to reach the target locations. In contrast, due to
PREDATOR’s directed fuzzing mechanism, it can quickly
identify promising parameter keys and values.