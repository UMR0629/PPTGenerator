Hvaluation Dataset. We select applications evaluated in
prior studies [6, 12, 15, 16, 30] as our dataset for evaluation.
These encompass 3 synthetic test suites and 11 real-world
applications, comprising around 46K files and 2M LoC in
total. They collectively contain a total of 51 SQL injection
(SQLi), 8 command injection (CMDi), and 26 reflected cross-
site scripting (XSS) vulnerabilities. We manually collect
known vulnerabilities from the CVE database [37] in a
best effort manner, yet we acknowledge the possibility of
omissions. We exclude applications that only run on PHP 5,
which has long been obsolete and is no longer supported by
the PHP community. For these applications, we believe that
the benefit of detecting their vulnerabilities is not significant.

TA tha hact nf nny brunawladan nn avicting nn aAaNnnnmNm
To the best of our knowledge, no existing fuzzer encom-
passes a vulnerability scope identical to that of PREDATOR.
Therefore, we compare PREDATOR with various state-of-
the-art fuzzers for different types of vulnerabilities, includ-
ing Witcher [16] and Atropos [18] for SQLi and CMDi
vulnerabilities, Cefuzz [30] for CMDi vulnerabilities, and
WebFuzz [15] for XSS vulnerabilities. To better compare the
performance of PREDATOR with Witcher, we add the XSS
detector to Witcher and denote it as Witcher+. The source
code of Atropos—a concurrent work—is not available at
the time of writing. Cefuzz is closed-source and we are not
able to run it for a direct comparison. We thus borrow the
evaluation results presented in the papers [18, 30] of Atropos
and Cefuzz for a comparison. We manually check the testing
results generated by each tool to confirm if they are true
positives.