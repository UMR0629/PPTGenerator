TABLE 2: The evaluation results on synthetic test suites. We list the total number of vulnerabilities by type, in the format of
SQLi + CMDi + XSS. As the source code is not available at the time of writing, we borrow the detection results from the papers of
Atropos and Cefuzz. "-" denotes that no detection results are available.
TABLE 3: The evaluation results on real-world applications. We list the total number of vulnerabilities by type, in the format of
SQLi + CMDi + XSS. +We remove the hidden tokens in WeBid for a comparison between PREDATOR and Witcher.
based approach, making the random tokens deterministic
during fuzzing [18]. We cite Atroposâ€™ detection results in the
best-case scenario as the ground truths used in the evaluation
of Atropos and PREDATOR are not identical. When evaluating
PREDATOR on reproducing known vulnerabilities, we did
not count any true-positive vulnerabilities outside the ground
truth, whereas Atropos did. The current testing results for
Atropos are limited to three synthetic test suites, which
hinders our ability to perform a comprehensive comparison
on real-world applications.
Comparison with YWebdruzzZ ine performance of
PREDATOR is slightly better than that of WebFuzz. We
noticed that WebFuzz occasionally got stuck on certain
pages, preventing further progress. For instance, when testing
Hospital Management System, despite providing valid login
credentials, WebFuzz consistently attempted to visit inacces-
sible pages. This likely represents a design or implementation
flaw, leading it to waste considerable time exploring irrelevant
paths. When testing other applications, its performance
was satisfactory yet remained marginally inferior to that
of PREDATOR.

False Negatives. We analyze the reasons behind the
false negatives. In bWAPP, 6 SQLi vulnerabilities were not
detected, 3 of which were due to the lack of oracle for sqlite.
This could be easily addressed by detecting the correspond-
ing error feedback messages. However, we refrained from
improving it to maintain fairness in comparison with Witcher.
This is one of the reasons why PREDATOR underperformed
Atropos in detecting SQLi vulnerabilities in bWAPP. The
remaining 3 vulnerabilities could not be triggered by our
current prototype due to the presence of CAPTCHAs and the
requirement of specific User Agent (UA) header values. For
a fair comparison with Atropos, we did not remove hidden
CSRF tokens in DVWA. As Witcher and PREDATOR cannot
handle hidden tokens, they failed to trigger any vulnerabilities
in DVWA. The vulnerabilities undetected in OpenEMR could
be identified by concurrently fuzzing with multiple cores or
increasing the time budget. Moreover, we discovered that
PREDATOR was unable to detect any vulnerabilities within
WordPress, Piwigo and Joomla due to the failure of entry
URL identification. For example, they dynamically includes
server-side files based on the values of query strings, which
are impractical to identify using static analysis methods.
During the XSS vulnerability detection, we noted that the
absence of injected payloads in the source code of some
pages prevented the matching of the attack string. As a
result, PREDATOR failed to detect the vulnerabilities. This
is an implementation limitation of the matching-based XSS
detection method.
False Positives. All 6 false positives were reported during
the XSS vulnerability detection. This is possibly due to the
high false-positive rate of the string matching-based XSS